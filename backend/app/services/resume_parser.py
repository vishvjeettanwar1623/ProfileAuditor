import PyPDF2
import docx
import re
from typing import Dict, Any, List, Optional
import os

# Improved PDF parsing libraries
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
    print("pdfplumber library loaded successfully")
except ImportError:
    PDFPLUMBER_AVAILABLE = False
    print("pdfplumber not available")

try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
    print("PyMuPDF library loaded successfully")
except ImportError:
    PYMUPDF_AVAILABLE = False
    print("PyMuPDF not available")

try:
    from pdfminer.high_level import extract_text as pdfminer_extract_text
    PDFMINER_AVAILABLE = True
    print("pdfminer.six library loaded successfully")
except ImportError:
    PDFMINER_AVAILABLE = False
    print("pdfminer.six not available")

# Lightweight NLP fallback for cloud deployments
class SimpleNLP:
    def __call__(self, text):
        return SimpleDoc(text)
    
    def __getattr__(self, name):
        return lambda *args, **kwargs: None

class SimpleDoc:
    def __init__(self, text):
        self.text = text
        self.ents = []
    
    def __getattr__(self, name):
        return []

# Try to load spaCy with fallback for cloud deployments
nlp = None
try:
    import spacy
    # Try to use a blank English model first (smaller)
    try:
        nlp = spacy.blank("en")
        print("Using spaCy blank English model")
    except:
        # Fallback to full model if available
        try:
            nlp = spacy.load("en_core_web_sm")
            print("Using spaCy full English model")
        except:
            raise Exception("No spaCy model available")
except Exception as e:
    print(f"SpaCy not available, using simple NLP fallback: {e}")
    nlp = SimpleNLP()

def parse_resume(file_path: str) -> Dict[str, Any]:
    """Parse resume file and extract information"""
    # Extract text based on file type
    if file_path.endswith(".pdf"):
        text = extract_text_from_pdf(file_path)
    elif file_path.endswith((".doc", ".docx")):
        text = extract_text_from_docx(file_path)
    else:
        raise ValueError("Unsupported file format")
    
    # Extract information from text
    parsed_data = extract_information(text)
    
    return parsed_data

from typing import Tuple

def parse_resume_with_text(file_path: str) -> Tuple[Dict[str, Any], str]:
    """Parse resume file, returning both structured data and the raw extracted text."""
    if file_path.endswith(".pdf"):
        text = extract_text_from_pdf(file_path)
    elif file_path.endswith((".doc", ".docx")):
        text = extract_text_from_docx(file_path)
    else:
        raise ValueError("Unsupported file format")
    parsed_data = extract_information(text)
    return parsed_data, text

def extract_text_from_pdf(file_path: str) -> str:
    """Extract text from PDF file using multiple fallback libraries for best results"""
    print(f"\n=== PDF EXTRACTION START ===")
    print(f"Extracting text from PDF: {file_path}")
    
    text = ""
    
    # Try Method 1: pdfplumber (best for complex layouts)
    if PDFPLUMBER_AVAILABLE:
        try:
            print("Trying pdfplumber extraction...")
            with pdfplumber.open(file_path) as pdf:
                extracted_text = ""
                for page_num, page in enumerate(pdf.pages):
                    page_text = page.extract_text() or ""
                    print(f"pdfplumber Page {page_num + 1}: {len(page_text)} chars")
                    extracted_text += page_text + "\n"
                
                if extracted_text.strip():
                    text = extracted_text
                    print(f"âœ… pdfplumber extraction successful: {len(text)} chars")
                else:
                    print("âŒ pdfplumber extraction returned empty text")
        except Exception as e:
            print(f"âŒ pdfplumber extraction failed: {str(e)}")
    
    # Try Method 2: PyMuPDF if pdfplumber failed
    if not text and PYMUPDF_AVAILABLE:
        try:
            print("Trying PyMuPDF extraction...")
            doc = fitz.open(file_path)
            extracted_text = ""
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text()
                print(f"PyMuPDF Page {page_num + 1}: {len(page_text)} chars")
                extracted_text += page_text + "\n"
            doc.close()
            
            if extracted_text.strip():
                text = extracted_text
                print(f"âœ… PyMuPDF extraction successful: {len(text)} chars")
            else:
                print("âŒ PyMuPDF extraction returned empty text")
        except Exception as e:
            print(f"âŒ PyMuPDF extraction failed: {str(e)}")
    
    # Try Method 3: pdfminer if others failed
    if not text and PDFMINER_AVAILABLE:
        try:
            print("Trying pdfminer extraction...")
            extracted_text = pdfminer_extract_text(file_path)
            if extracted_text and extracted_text.strip():
                text = extracted_text
                print(f"âœ… pdfminer extraction successful: {len(text)} chars")
            else:
                print("âŒ pdfminer extraction returned empty text")
        except Exception as e:
            print(f"âŒ pdfminer extraction failed: {str(e)}")
    
    # Fallback Method 4: PyPDF2 (last resort)
    if not text:
        try:
            print("Trying PyPDF2 extraction (fallback)...")
            with open(file_path, "rb") as file:
                reader = PyPDF2.PdfReader(file)
                extracted_text = ""
                for page_num, page in enumerate(reader.pages):
                    try:
                        page_text = page.extract_text() or ""
                        print(f"PyPDF2 Page {page_num + 1}: {len(page_text)} chars")
                        extracted_text += page_text + "\n"
                    except Exception as e:
                        print(f"Error extracting PyPDF2 page {page_num + 1}: {str(e)}")
                        continue
                
                if extracted_text.strip():
                    text = extracted_text
                    print(f"âœ… PyPDF2 extraction successful: {len(text)} chars")
                else:
                    print("âŒ PyPDF2 extraction returned empty text")
        except Exception as e:
            print(f"âŒ PyPDF2 extraction failed: {str(e)}")
    
    if not text:
        print("âŒ ALL PDF extraction methods failed!")
        return ""
    
    # Clean the extracted text
    print("ðŸ§¹ Cleaning extracted text...")
    text = clean_pdf_text(text)
    
    # Show sample for debugging projects section
    if "PROJECTS" in text.upper():
        project_start = text.upper().find("PROJECTS")
        if project_start >= 0:
            sample = text[project_start:project_start + 500]
            print(f"PROJECTS section sample: {repr(sample[:200])}...")
    
    print(f"=== PDF EXTRACTION COMPLETE ===")
    print(f"Final cleaned text: {len(text)} characters")
    
    return text.strip()

def clean_pdf_text(text: str) -> str:
    """Clean and normalize PDF extracted text"""
    if not text:
        return ""
    
    # Remove null characters
    text = text.replace('\x00', ' ')
    
    # Normalize Unicode characters (preserve em-dashes and special chars)
    # Convert various dash types to em-dash for consistency
    text = text.replace('â€“', 'â€”')  # en-dash to em-dash
    text = text.replace('âˆ’', 'â€”')  # minus to em-dash
    
    # Fix common PDF extraction issues while preserving important formatting
    # Remove excessive whitespace but preserve line breaks
    text = re.sub(r'[ \t]+', ' ', text)  # Multiple spaces/tabs to single space
    text = re.sub(r'\n[ \t]+', '\n', text)  # Remove leading whitespace on lines
    text = re.sub(r'[ \t]+\n', '\n', text)  # Remove trailing whitespace on lines
    text = re.sub(r'\n{3,}', '\n\n', text)  # Multiple newlines to double newline
    
    # Fix broken words (but be more conservative to avoid breaking intentional formatting)
    # Only fix obvious cases where single letters are separated
    text = re.sub(r'\b([a-zA-Z])\s+([a-zA-Z])\s+([a-zA-Z]{2,})\b', r'\1\2\3', text)
    
    return text.strip()

def extract_text_from_docx(file_path: str) -> str:
    """Extract text from DOCX file with improved handling of formatting"""
    print(f"Extracting text from DOCX: {file_path}")
    
    try:
        doc = docx.Document(file_path)
        text = ""
        
        # Extract from paragraphs
        for para in doc.paragraphs:
            para_text = para.text.strip()
            if para_text:
                text += para_text + "\n"
        
        # Extract from tables (if any)
        for table in doc.tables:
            for row in table.rows:
                row_text = []
                for cell in row.cells:
                    cell_text = cell.text.strip()
                    if cell_text:
                        row_text.append(cell_text)
                if row_text:
                    text += " ".join(row_text) + "\n"
        
        print(f"DOCX: Successfully extracted {len(text)} characters")
        return text.strip()
        
    except Exception as e:
        print(f"DOCX extraction failed: {e}")
        return ""

def extract_information(text: str) -> Dict[str, Any]:
    """Extract structured information from resume text"""
    print(f"Extracting information from text of length: {len(text)}")
    
    # Process text with spaCy
    try:
        doc = nlp(text)
        print("Successfully processed text with spaCy")
    except Exception as e:
        print(f"Error processing text with spaCy: {str(e)}")
        doc = text  # Fallback to using raw text
    
    # Initialize result dictionary
    result = {
        "name": None,
        "email": None,
        "phone": None,
        "skills": [],
        "projects": [],
        "experience": [],
        "education": [],
        "github_username": None,
        "twitter_username": None,
        "linkedin_username": None,
        "status": "completed"  # Ensure status is set
    }
    
    # Extract information
    try:
        result["name"] = extract_name(doc)
        print(f"Extracted name: {result['name']}")
        
        result["email"] = extract_email(text)
        print(f"Extracted email: {result['email']}")
        
        result["phone"] = extract_phone(text)
        print(f"Extracted phone: {result['phone']}")
        
        result["skills"] = extract_skills(doc, text)
        print(f"Extracted skills: {result['skills']}")
        
        # Extract additional skills from achievements and extracurricular activities
        additional_skills = extract_additional_skills_from_achievements(text)
        if additional_skills:
            result["skills"].extend(additional_skills)
            result["skills"] = list(set(result["skills"]))  # Remove duplicates
            print(f"Added skills from achievements/extracurricular: {additional_skills}")
        
        result["projects"] = extract_projects(doc, text)
        print(f"Extracted projects: {result['projects']}")
        
        result["experience"] = extract_experience(doc, text)
        print(f"Extracted experience count: {len(result['experience'])}")
        
        result["education"] = extract_education(doc, text)
        print(f"Extracted education count: {len(result['education'])}")
        
        result["github_username"] = extract_github_username(text)
        print(f"Extracted github_username: {result['github_username']}")
        if result['github_username']:
            print(f"âœ… GitHub username found: {result['github_username']}")
        else:
            print(f"âŒ No GitHub username found in resume text")
        
        result["twitter_username"] = extract_twitter_username(text)
        print(f"Extracted twitter_username: {result['twitter_username']}")
        if result['twitter_username']:
            print(f"âœ… Twitter username found: {result['twitter_username']}")
        else:
            print(f"âŒ No Twitter username found in resume text")
        
        result["linkedin_username"] = extract_linkedin_username(text)
        print(f"Extracted linkedin_username: {result['linkedin_username']}")
        if result['linkedin_username']:
            print(f"âœ… LinkedIn username found: {result['linkedin_username']}")
        else:
            print(f"âŒ No LinkedIn username found in resume text")
    except Exception as e:
        print(f"Error during information extraction: {str(e)}")
    
    # Ensure we have at least some skills for verification
    if not result["skills"]:
        print("No skills extracted, adding default skills")
        result["skills"] = ["Python", "Data Analysis", "Communication"]
    
    # Do not inject default projects; keep empty if none extracted so verification uses resume-derived projects only
    return result

def extract_name(doc) -> Optional[str]:
    """Extract name from spaCy doc"""
    # Check if doc is a spaCy doc with entities
    if hasattr(doc, 'ents') and doc.ents:
        # Look for PERSON entities at the beginning of the document
        for ent in doc.ents:
            if hasattr(ent, 'label_') and ent.label_ == "PERSON":
                return ent.text
    
    # Fallback: Try to extract name from the first few lines
    if isinstance(doc, str):
        lines = doc.split('\n')
        for line in lines[:5]:  # Check first 5 lines
            if line.strip() and not re.match(r'\b(?:email|phone|address|resume)\b', line.lower()):
                return line.strip()
    
    return None

def extract_email(text: str) -> Optional[str]:
    """Extract email from text"""
    email_pattern = r'[\w\.-]+@[\w\.-]+\.[a-zA-Z]{2,}'
    match = re.search(email_pattern, text)
    if match:
        return match.group(0)
    return None

def extract_phone(text: str) -> Optional[str]:
    """Extract phone number from text"""
    # Various phone patterns
    phone_patterns = [
        r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b',  # 123-456-7890
        r'\b\(\d{3}\)[-. ]?\d{3}[-.]?\d{4}\b',  # (123) 456-7890
        r'\b\+\d{1,2}[-. ]?\d{3}[-.]?\d{3}[-.]?\d{4}\b'  # +1 123-456-7890
    ]
    
    for pattern in phone_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(0)
    return None

def extract_skills(doc, text: str) -> List[str]:
    """Extract skills from text"""
    # Ensure we have text to work with
    if not text and isinstance(doc, str):
        text = doc
    # Common skill keywords and programming languages
    skill_keywords = [
        "Python", "JavaScript", "Java", "C++", "C#", "Ruby", "PHP", "Swift", "Kotlin",
        "React", "Angular", "Vue", "Node.js", "Express", "Django", "Flask", "Spring",
        "TensorFlow", "PyTorch", "Machine Learning", "Deep Learning", "AI", "NLP",
        "SQL", "MySQL", "PostgreSQL", "MongoDB", "Firebase", "AWS", "Azure", "GCP",
        "Docker", "Kubernetes", "CI/CD", "Git", "GitHub", "DevOps", "Agile", "Scrum",
        "HTML", "CSS", "SASS", "LESS", "Bootstrap", "Tailwind", "Material UI",
        "REST API", "GraphQL", "Microservices", "Serverless", "Linux", "Unix",
        "Data Analysis", "Data Science", "Big Data", "Hadoop", "Spark", "Tableau",
        "Power BI", "Excel", "VBA", "R", "MATLAB", "SAS", "SPSS", "Stata",
        "TypeScript", "Go", "Rust", "Scala", "Perl", "COBOL", "Fortran",
        "jQuery", "Redux", "Next.js", "Gatsby", "Svelte", "Ember", "Backbone",
        "Laravel", "CodeIgniter", "Symfony", "Rails", "Sinatra", "ASP.NET",
        "Pandas", "NumPy", "Scikit-learn", "Keras", "OpenCV", "NLTK", "SpaCy",
        "Redis", "Elasticsearch", "Cassandra", "DynamoDB", "Oracle", "SQLite",
        "Jenkins", "GitLab", "CircleCI", "Travis", "Ansible", "Terraform", "Vagrant",
        "Figma", "Sketch", "Adobe", "Photoshop", "Illustrator", "InDesign", "XD",
        "Project Management", "Leadership", "Communication", "Problem Solving", "Teamwork",
        "C", "Solidity", "VS Code", "Canva", "Notion", "Public Speaking", "Community Management"
    ]
    
    # Look for skill keywords in text (only from dedicated skills sections)
    found_skills = []
    
    print(f"=== SKILLS EXTRACTION START ===")
    print(f"Text length: {len(text)}")
    
    # Look for skill sections with more flexible patterns - but be very strict about what we extract
    skill_section_patterns = [
        r'(?i)(?:technical\s+)?skills?\s*[:\n]\s*(.*?)(?=\n\s*(?:PROJECTS?|EXPERIENCE|EDUCATION|WORK\s+EXPERIENCE|EMPLOYMENT|ACHIEVEMENTS?|AWARDS?|CERTIFICATIONS?|REFERENCES?|CONTACT|SUMMARY|OBJECTIVE|LANGUAGES?|INTERESTS?|HOBBIES?|EXTRACURRICULAR|ACTIVITIES|VOLUNTEER|LEADERSHIP|SOCIAL\s+HANDLES?)\s*[:\n]|$)',
        r'(?i)programming\s*[:\n]\s*(.*?)(?=\n\s*(?:tools?|soft\s+skills?|languages?|projects?|experience|education)\s*[:\n]|$)',
        r'(?i)tools?\s*[:\n]\s*(.*?)(?=\n\s*(?:soft\s+skills?|languages?|projects?|experience|education|social\s+handles?)\s*[:\n]|$)',
        r'(?i)soft\s+skills?\s*[:\n]\s*(.*?)(?=\n\s*(?:languages?|projects?|experience|education|social\s+handles?)\s*[:\n]|$)',
        r'(?i)competencies[:\n]\s*(.*?)(?=\n\s*(?:PROJECTS?|EXPERIENCE|EDUCATION|WORK\s+EXPERIENCE|EMPLOYMENT|ACHIEVEMENTS?|AWARDS?|CERTIFICATIONS?|REFERENCES?|CONTACT|SUMMARY|OBJECTIVE|LANGUAGES?|INTERESTS?|HOBBIES?|EXTRACURRICULAR|ACTIVITIES|VOLUNTEER|LEADERSHIP|SOCIAL\s+HANDLES?)\s*[:\n]|$)',
        r'(?i)technologies[:\n]\s*(.*?)(?=\n\s*(?:PROJECTS?|EXPERIENCE|EDUCATION|WORK\s+EXPERIENCE|EMPLOYMENT|ACHIEVEMENTS?|AWARDS?|CERTIFICATIONS?|REFERENCES?|CONTACT|SUMMARY|OBJECTIVE|LANGUAGES?|INTERESTS?|HOBBIES?|EXTRACURRICULAR|ACTIVITIES|VOLUNTEER|LEADERSHIP|SOCIAL\s+HANDLES?)\s*[:\n]|$)',
        r'(?i)programming\s+languages?\s*[:\n]\s*(.*?)(?=\n\s*(?:PROJECTS?|EXPERIENCE|EDUCATION|WORK\s+EXPERIENCE|EMPLOYMENT|ACHIEVEMENTS?|AWARDS?|CERTIFICATIONS?|REFERENCES?|CONTACT|SUMMARY|OBJECTIVE|LANGUAGES?|INTERESTS?|HOBBIES?|EXTRACURRICULAR|ACTIVITIES|VOLUNTEER|LEADERSHIP|SOCIAL\s+HANDLES?)\s*[:\n]|$)',
        r'(?i)tools?\s+and\s+technologies[:\n]\s*(.*?)(?=\n\s*(?:PROJECTS?|EXPERIENCE|EDUCATION|WORK\s+EXPERIENCE|EMPLOYMENT|ACHIEVEMENTS?|AWARDS?|CERTIFICATIONS?|REFERENCES?|CONTACT|SUMMARY|OBJECTIVE|LANGUAGES?|INTERESTS?|HOBBIES?|EXTRACURRICULAR|ACTIVITIES|VOLUNTEER|LEADERSHIP|SOCIAL\s+HANDLES?)\s*[:\n]|$)'
    ]
    
    # First, look for skills only from predefined skill keywords in the text
    for skill in skill_keywords:
        if re.search(r'\b' + re.escape(skill) + r'\b', text, re.IGNORECASE):
            found_skills.append(skill)
    
    # Then, look for additional skills only in dedicated skills sections
    for pattern in skill_section_patterns:
        skill_matches = re.findall(pattern, text, re.DOTALL)
        if skill_matches:
            print(f"Found skills section with pattern: {len(skill_matches)} matches")
            for match in skill_matches:
                # Split by common separators including bullets, commas, pipes, newlines
                skills_text = match.strip()
                print(f"Processing skills text: {skills_text[:200]}...")
                
                # Split by various separators
                skills_list = re.split(r'[,â€¢|â€¢\n\t;/\\Â·â–ªâ–«â—¦â€£âƒ]+', skills_text)
                for skill in skills_list:
                    skill = re.sub(r'^[^\w]+|[^\w]+$', '', skill.strip())  # Remove leading/trailing non-word chars
                    skill = skill.strip()
                    
                    if skill and len(skill) > 1 and skill not in found_skills:
                        # Very strict filtering - only allow actual skills
                        if (skill.lower() not in ['and', 'or', 'with', 'using', 'including', 'etc', 'the', 'of', 'in', 'languages', 'english', 'hindi', 'skills', 'programming', 'tools', 'soft'] and
                            # Exclude project-like names
                            not any(keyword in skill.lower() for keyword in ['platform', 'app', 'website', 'portal', 'system', 'dashboard', 'service', 'roots', 'questfi', 'auditor', 'network', 'data sharing', 'monetization', 'bounty', 'verification', 'reality score']) and
                            # Exclude things that look like project descriptions
                            not re.search(r'â€”|â€“|-.*(?:platform|app|website|system|tool)', skill.lower()) and
                            # Exclude social media handles and usernames
                            not re.search(r'@\w+|linkedin\.com|github\.com|twitter\.com|vishvjeet|tanwar|vishvjeettanwar|1623|gmail\.com', skill.lower()) and
                            # Exclude URLs and links
                            not re.search(r'https?://|www\.|\.com|\.org|\.net|\[link\]|github\s*-\s*https|twitter\s*-\s*https|linkedin\s*-\s*https', skill.lower(), re.IGNORECASE) and
                            # Exclude very long strings that are likely descriptions
                            len(skill) < 30 and
                            # Exclude obvious non-skills
                            not re.search(r'\d{4}|\b(?:unverified|verified)\b', skill.lower()) and
                            # Don't exclude common soft skills
                            (skill in skill_keywords or 
                             skill.lower() in ['leadership', 'public speaking', 'community management', 'problem solving', 'teamwork', 'communication'] or
                             any(tech in skill.lower() for tech in ['programming', 'development', 'design', 'analysis', 'management', 'testing', 'deployment', 'script', 'code']))):
                            found_skills.append(skill)
                            print(f"âœ… Added skill: {skill}")
                        else:
                            print(f"âŒ Rejected skill: {skill}")
    
    # Remove duplicates while preserving order
    unique_skills = []
    seen = set()
    for skill in found_skills:
        if skill.lower() not in seen:
            unique_skills.append(skill)
            seen.add(skill.lower())
    
    print(f"=== SKILLS EXTRACTION COMPLETE ===")
    print(f"Found {len(unique_skills)} skills: {unique_skills}")
    
    return unique_skills

def fix_pdf_extraction_issues(text: str) -> str:
    """Fix common PDF extraction issues that break project names"""
    if not text:
        return text
    
    print("ðŸ”§ Fixing PDF extraction issues...")
    
    # Fix specific broken project names first (most important)
    fixes = {
        # Project name fixes
        'Ques tfi': 'Questfi',
        'Quest fi': 'Questfi', 
        'Que stfi': 'Questfi',
        'Profile Audit or': 'Profile Auditor',
        'Profile Audit': 'Profile Auditor',  # if 'or' is on next line
        'Data\nRoots': 'Data Roots',
        'Data \nRoots': 'Data Roots',
        'Data\n Roots': 'Data Roots',
        
        # Common broken words
        'block chain': 'blockchain',
        'Block chain': 'Blockchain',
        'real ity': 'reality',
        'Real ity': 'Reality',
        'plat form': 'platform',
        'Plat form': 'Platform',
        'net work': 'network',
        'Net work': 'Network',
        'data base': 'database',
        'Data base': 'Database',
        'web site': 'website',
        'Web site': 'Website',
        'app lication': 'application',
        'App lication': 'Application',
        'ver ification': 'verification',
        'Ver ification': 'Verification',
        'monetiz ation': 'monetization',
        'Monetiz ation': 'Monetization',
        'decent ralized': 'decentralized',
        'Decent ralized': 'Decentralized',
        'shar ing': 'sharing',
        'Shar ing': 'Sharing'
    }
    
    # Apply specific fixes
    for broken, fixed in fixes.items():
        if broken in text:
            text = text.replace(broken, fixed)
            print(f"  Fixed: '{broken}' â†’ '{fixed}'")
    
    # Fix broken words using regex (more general patterns)
    # Pattern: single letter followed by space and rest of word
    text = re.sub(r'\b([A-Z])ues tfi\b', r'\1uestfi', text)  # Specific for Questfi variants
    text = re.sub(r'\bQues ([a-z]fi)\b', r'Quest\1', text)   # Questfi variants
    
    # General pattern for broken compound words (be conservative)
    # Only fix obvious cases where we have a capital + lowercase fragment
    text = re.sub(r'\b([A-Z][a-z]+)\s+([a-z]{2,})\b(?=\s+(?:[A-Z]|â€”|-|$))', r'\1\2', text)
    
    return text

def extract_projects(doc_or_text, text: str = None) -> List[Dict[str, Any]]:
    """Extract projects from text - supports both (doc, text) and (text) calling patterns"""
    # Handle different calling patterns
    if text is None:
        # Called with single parameter: extract_projects(text)
        if isinstance(doc_or_text, str):
            text = doc_or_text
            doc = None
        else:
            # Called with spaCy doc only
            doc = doc_or_text
            text = str(doc) if hasattr(doc, 'text') else str(doc)
    else:
        # Called with two parameters: extract_projects(doc, text)
        doc = doc_or_text
        if not text and isinstance(doc, str):
            text = doc
    projects = []
    
    print(f"\n=== PROJECT EXTRACTION START ===")
    print(f"Text length: {len(text)}")
    print(f"Text contains 'PROJECTS': {'PROJECTS' in text.upper()}")
    print(f"Text contains 'Data Roots': {'Data Roots' in text}")
    print(f"Text contains 'Questfi': {'Questfi' in text}")
    print(f"Text contains broken 'Ques tfi': {'Ques tfi' in text}")
    print(f"Text contains broken 'Profile Audit or': {'Profile Audit or' in text}")
    print(f"First 500 chars: {text[:500]}")
    
    # Fix common PDF extraction issues BEFORE pattern matching
    text = fix_pdf_extraction_issues(text)
    print(f"After PDF fixes - Text contains 'Questfi': {'Questfi' in text}")
    print(f"After PDF fixes - Text contains 'Profile Auditor': {'Profile Auditor' in text}")
    
    # Enhanced pattern matching for PDF format
    # Support multiple dash variations and space patterns that PDF extraction creates
    em_dash_patterns = [
        r'(?:^|\n)\s*([A-Z][A-Za-z0-9\s,-]{2,50})\s*â€”\s*([^[\n]+?)(?:\s*\[.*?\])?\.?\s*(?=\n|$)',  # em-dash
        r'(?:^|\n)\s*([A-Z][A-Za-z0-9\s,-]{2,50})\s*[-â€“]\s*([^[\n]+?)(?:\s*\[.*?\])?\.?\s*(?=\n|$)',  # en-dash or hyphen
        r'(?:^|\n)\s*([A-Z][A-Za-z0-9\s,-]{2,50})\s{3,}\s*([^[\n]+?)(?:\s*\[.*?\])?\.?\s*(?=\n|$)'  # multiple spaces (PDF conversion)
    ]
    
    total_em_dash_matches = []
    for i, pattern in enumerate(em_dash_patterns):
        matches = re.findall(pattern, text, re.MULTILINE)
        print(f"Pattern {i+1} found {len(matches)} matches")
        total_em_dash_matches.extend(matches)
    
    print(f"Found {len(total_em_dash_matches)} total dash/space matches")
    
    for match in total_em_dash_matches:
        project_name = match[0].strip()
        project_desc = match[1].strip()
        
        print(f"Checking dash match: '{project_name}' â€” '{project_desc}'")
        
        # Skip if project name contains section headers
        if any(header in project_name.upper() for header in ['PROJECTS', 'EXPERIENCE', 'EDUCATION', 'SKILLS', 'ACHIEVEMENTS', 'INTERNSHIP', 'AWARDS']):
            print(f"âŒ Rejected dash match: '{project_name}' (contains section header)")
            continue
            
        # Skip if this looks like achievements content (more comprehensive check)
        achievement_keywords = ['prize', 'award', 'hackathon', 'competition', 'winner', 'achievement', 'certificate', 'honor', 'built a blockchain', 'engaged in a national', 'demonstrating problem-solving', '3rd prize', '30th position', 'completed a 21', 'algohour', 'regional hackathon']
        social_keywords = ['twitter', 'gmail', 'linkedin', 'github', 'email', 'phone', '.com', '@', 'www.', 'http', 'https']
        experience_keywords = ['intern', 'internship', 'remote', 'present', 'hackquest', 'offscript', 'advocate', 'building and managing', 'promoting web3']
        
        combined_text = (project_name + " " + project_desc).lower()
        
        if any(keyword in combined_text for keyword in achievement_keywords):
            print(f"âŒ Rejected dash match: '{project_name}' (achievements content)")
            continue
            
        if any(keyword in combined_text for keyword in social_keywords):
            print(f"âŒ Rejected dash match: '{project_name}' (social media/contact content)")
            continue
            
        if any(keyword in combined_text for keyword in experience_keywords):
            print(f"âŒ Rejected dash match: '{project_name}' (experience/work content)")
            continue
            
        # Skip if this appears to be from achievements section (check context)
        # Look for achievement markers before the match
        match_pos = text.find(project_name)
        if match_pos > 0:
            preceding_text = text[max(0, match_pos - 200):match_pos].lower()
            if any(keyword in preceding_text for keyword in ['achievements', 'extra-curricular', 'hackathon', 'competition', 'prize']):
                print(f"âŒ Rejected dash match: '{project_name}' (found in achievements context)")
                continue
        
        # Validate this looks like a project
        if (len(project_name) >= 3 and len(project_name) <= 60 and 
            project_name[0].isupper() and
            # Exclude obvious non-projects
            not any(keyword in project_name.lower() for keyword in ['achievements', 'awards', 'honors', 'experience', 'education', 'skills', 'certifications', 'contact', 'summary', 'languages', 'interests']) and
            # Include if it has project-like keywords OR is a proper noun
            (any(keyword in (project_name + " " + project_desc).lower() for keyword in ['platform', 'app', 'website', 'system', 'tool', 'network', 'blockchain', 'data', 'bounty', 'verification', 'auditor', 'sharing', 'monetization']) or
             (len(project_name.split()) <= 4 and not any(word in project_name.lower() for word in ['and', 'or', 'the', 'of', 'in', 'at', 'on', 'for'])))):
            
            projects.append({
                "name": project_name,
                "description": project_desc
            })
            print(f"âœ… Added dash project: '{project_name}' â€” '{project_desc}'")
        else:
            print(f"âŒ Rejected dash match: '{project_name}' (validation failed)")
    
    # Also look for projects in a dedicated PROJECTS section (but exclude achievements section)
    project_section_patterns = [
        r'(?i)projects?\s*[:\n]\s*(.*?)(?=\n\s*(?:ACHIEVEMENTS?\s*&\s*EXTRA|ACHIEVEMENTS?|AWARDS?|SKILLS?|EXPERIENCE|EDUCATION|WORK\s+EXPERIENCE|EMPLOYMENT|CERTIFICATIONS?|REFERENCES?|CONTACT|SUMMARY|OBJECTIVE|LANGUAGES?|INTERESTS?|HOBBIES?|EXTRACURRICULAR|ACTIVITIES|VOLUNTEER|LEADERSHIP|SOCIAL\s+HANDLES?)\s*[:\n]|$)'
    ]
    
    for pattern in project_section_patterns:
        project_matches = re.findall(pattern, text, re.DOTALL)
        print(f"Pattern found {len(project_matches)} matches")
        
        if project_matches:
            for match in project_matches:
                project_text = match.strip()
                print(f"Processing project section ({len(project_text)} chars): {project_text[:200]}...")
                
                # Look for em-dash projects within this section with improved regex (all dash variations)
                section_matches = []
                for pattern_variant in em_dash_patterns:
                    variant_matches = re.findall(pattern_variant, project_text, re.MULTILINE)
                    section_matches.extend(variant_matches)
                    
                for em_match in section_matches:
                    project_name = em_match[0].strip()
                    project_desc = em_match[1].strip()
                    
                    # Clean project name by removing [Link] annotations
                    project_name = re.sub(r'\[.*?\]', '', project_name).strip()
                    
                    # Skip achievements content
                    if any(keyword in (project_name + " " + project_desc).lower() for keyword in ['prize', 'award', 'hackathon', 'competition', 'winner', 'achievement', 'certificate', 'honor']):
                        print(f"âŒ Rejected section match: '{project_name}' (achievements content)")
                        continue
                    
                    # Skip achievements content in section matches too
                    achievement_keywords = ['prize', 'award', 'hackathon', 'competition', 'winner', 'achievement', 'certificate', 'honor', 'built a blockchain', 'engaged in a national', 'demonstrating problem-solving', '3rd prize', '30th position', 'completed a 21', 'algohour', 'regional hackathon']
                    social_keywords = ['twitter', 'gmail', 'linkedin', 'github', 'email', 'phone', '.com', '@', 'www.', 'http', 'https']
                    experience_keywords = ['intern', 'internship', 'remote', 'present', 'hackquest', 'offscript', 'advocate', 'building and managing', 'promoting web3']
                    
                    combined_text = (project_name + " " + project_desc).lower()
                    
                    if any(keyword in combined_text for keyword in achievement_keywords):
                        print(f"âŒ Rejected section match: '{project_name}' (achievements content)")
                        continue
                        
                    if any(keyword in combined_text for keyword in social_keywords):
                        print(f"âŒ Rejected section match: '{project_name}' (social media/contact content)")
                        continue
                        
                    if any(keyword in combined_text for keyword in experience_keywords):
                        print(f"âŒ Rejected section match: '{project_name}' (experience/work content)")
                        continue
                    
                    # Check if this appears to be from achievements section by checking context
                    match_pos = project_text.find(project_name)
                    if match_pos > 0:
                        preceding_text = project_text[max(0, match_pos - 200):match_pos].lower()
                        if any(keyword in preceding_text for keyword in ['achievements', 'extra-curricular', 'hackathon', 'competition', 'prize']):
                            print(f"âŒ Rejected section match: '{project_name}' (found in achievements context)")
                            continue
                    
                    if project_name and len(project_name) >= 3:
                        # Check if we already have this project
                        existing_names = [p['name'].lower() for p in projects]
                        if project_name.lower() not in existing_names:
                            projects.append({
                                "name": project_name,
                                "description": project_desc
                            })
                            print(f"âœ… Added section project: '{project_name}'")
                
                # Also try other parsing methods for this section
                parsed_projects = parse_project_section(project_text)
                for proj in parsed_projects:
                    existing_names = [p['name'].lower() for p in projects]
                    if proj['name'].lower() not in existing_names:
                        # Skip achievements content in parsed projects too
                        achievement_keywords = ['prize', 'award', 'hackathon', 'competition', 'winner', 'achievement', 'certificate', 'honor', 'built a blockchain', 'engaged in a national', 'demonstrating problem-solving', '3rd prize', '30th position', 'completed a 21', 'algohour', 'regional hackathon']
                        social_keywords = ['twitter', 'gmail', 'linkedin', 'github', 'email', 'phone', '.com', '@', 'www.', 'http', 'https']
                        experience_keywords = ['intern', 'internship', 'remote', 'present', 'hackquest', 'offscript', 'advocate', 'building and managing', 'promoting web3']
                        
                        combined_text = (proj['name'] + " " + proj.get('description', '')).lower()
                        
                        if not any(keyword in combined_text for keyword in achievement_keywords + social_keywords + experience_keywords):
                            projects.append(proj)
                            print(f"âœ… Added parsed project: '{proj['name']}'")
                        else:
                            print(f"âŒ Rejected parsed project: '{proj['name']}' (unwanted content)")
                
                # Additional PDF-style parsing for this section (space-separated instead of dash-separated)
                print("Trying PDF-style parsing within projects section...")
                section_lines = project_text.split('\n')
                for line in section_lines:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # Remove [Link] annotations
                    clean_line = re.sub(r'\[.*?\]', '', line).strip()
                    
                    # Look for lines that contain project keywords but no dashes
                    if (any(keyword in clean_line.lower() for keyword in ['platform', 'app', 'website', 'system', 'tool', 'network', 'blockchain', 'data', 'bounty', 'verification', 'auditor', 'sharing', 'monetization']) and
                        'â€”' not in clean_line and '-' not in clean_line):
                        
                        # Try to split into name and description
                        words = clean_line.split()
                        if len(words) >= 3:
                            # Try different split points, preferring longer names for compound project names
                            best_split = None
                            best_score = 0
                            
                            for split_point in range(1, min(5, len(words))):  # Allow up to 4-word project names
                                potential_name = ' '.join(words[:split_point])
                                potential_desc = ' '.join(words[split_point:])
                                
                                # Score this split based on how well it separates name vs description
                                score = 0
                                
                                # Prefer names that are proper nouns (title case)
                                if all(word[0].isupper() for word in potential_name.split() if word):
                                    score += 2
                                
                                # Prefer descriptions that contain action/descriptive words
                                desc_words = potential_desc.lower().split()
                                if any(word in desc_words for word in ['decentralized', 'platform', 'app', 'application', 'system', 'tool', 'verification', 'generating', 'blockchain', 'network', 'bounty']):
                                    score += 1
                                
                                # Penalize very short names unless they're obviously complete
                                if len(potential_name.split()) == 1 and potential_name.lower() not in ['questfi', 'auditor', 'platform']:
                                    score -= 1
                                
                                # Prefer 2-word names for compound projects, but also accept single distinctive names
                                if len(potential_name.split()) == 2:
                                    score += 1
                                elif len(potential_name.split()) == 1 and potential_name.lower() in ['questfi']:
                                    score += 2  # Single names that are clearly project names
                                
                                if score > best_score and len(potential_name) >= 3 and len(potential_name) <= 30:
                                    best_score = score
                                    best_split = (potential_name, potential_desc)
                            
                            if (best_split and 
                                best_split[0][0].isupper() and
                                any(keyword in best_split[1].lower() for keyword in ['platform', 'app', 'website', 'system', 'tool', 'network', 'blockchain', 'data', 'bounty', 'verification', 'auditor', 'sharing', 'monetization'])):
                                
                                # Check if we already have this project
                                existing_names = [p['name'].lower() for p in projects]
                                if best_split[0].lower() not in existing_names:
                                    projects.append({
                                        "name": best_split[0],
                                        "description": best_split[1]
                                    })
                                    print(f"âœ… Added PDF-style project: '{best_split[0]}' â€” '{best_split[1]}'")
                                    # Don't break here - continue to find more projects
            break  # Only process the first matching pattern to avoid duplicates
    
    # If still no projects found, try to extract from the entire text using common project patterns
    if not projects:
        print("No projects found, trying full text extraction...")
        projects = extract_projects_from_full_text(text)
    
    # If STILL no projects found, try PDF-specific patterns (space-separated instead of dash-separated)
    if not projects:
        print("Trying PDF-specific space-separated patterns...")
        # Look for projects in format: "ProjectName Description with keywords Link"
        # Common in PDF extraction where dashes are lost
        project_lines = []
        lines = text.split('\n')
        in_projects_section = False
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Check if we're entering projects section
            if re.match(r'^\s*PROJECTS?\s*$', line, re.IGNORECASE):
                in_projects_section = True
                continue
                
            # Check if we're leaving projects section
            if in_projects_section and re.match(r'^\s*(?:ACHIEVEMENTS?|EXPERIENCE|EDUCATION|SKILLS|INTERNSHIP|SOCIAL\s+HANDLES?|LANGUAGES?)\s*', line, re.IGNORECASE):
                break
                
            # If we're in projects section, collect lines that might be projects
            if in_projects_section and line:
                # Look for lines that contain project-like keywords
                if any(keyword in line.lower() for keyword in ['platform', 'app', 'website', 'system', 'tool', 'network', 'blockchain', 'data', 'bounty', 'verification', 'auditor', 'sharing', 'monetization']):
                    project_lines.append(line)
        
        print(f"Found {len(project_lines)} potential PDF project lines")
        
        # Try to parse each project line
        for line in project_lines:
            # Remove [Link] annotations
            clean_line = re.sub(r'\[.*?\]', '', line).strip()
            
            # Try to split into name and description
            # Pattern: "ProjectName rest of description with keywords"
            words = clean_line.split()
            if len(words) >= 3:
                # Try different split points to find the project name
                for split_point in range(1, min(4, len(words))):
                    potential_name = ' '.join(words[:split_point])
                    potential_desc = ' '.join(words[split_point:])
                    
                    # Check if this split makes sense
                    if (len(potential_name) >= 3 and len(potential_name) <= 30 and
                        potential_name[0].isupper() and
                        any(keyword in potential_desc.lower() for keyword in ['platform', 'app', 'website', 'system', 'tool', 'network', 'blockchain', 'data', 'bounty', 'verification', 'auditor', 'sharing', 'monetization']) and
                        not any(keyword in potential_name.lower() for keyword in ['prize', 'award', 'hackathon', 'competition', 'twitter', 'gmail', 'linkedin', 'github', 'intern', 'experience'])):
                        
                        # Check if we already have this project
                        existing_names = [p['name'].lower() for p in projects]
                        if potential_name.lower() not in existing_names:
                            projects.append({
                                "name": potential_name,
                                "description": potential_desc
                            })
                            print(f"âœ… Added PDF project: '{potential_name}' â€” '{potential_desc}'")
                            break

    # Clean up and deduplicate projects
    cleaned_projects = []
    seen_names = set()
    
    for project in projects:
        name = project["name"].strip()
        # Clean up common prefixes/suffixes
        name = re.sub(r'^[-â€¢\s]+', '', name)  # Remove bullet points
        name = re.sub(r'[:\s]+$', '', name)   # Remove trailing colons/spaces
        name = re.sub(r'\[.*?\]', '', name).strip()  # Remove [Link] annotations
        
        if name and name.lower() not in seen_names and len(name) > 3:
            cleaned_projects.append({
                "name": name,
                "description": project.get("description", "")
            })
            seen_names.add(name.lower())
    
    print(f"=== PROJECT EXTRACTION COMPLETE ===")
    print(f"Found {len(cleaned_projects)} projects: {[p['name'] for p in cleaned_projects]}")
    
    return cleaned_projects

def parse_project_section(project_text: str) -> List[Dict[str, Any]]:
    """Parse projects from a dedicated project section"""
    projects = []
    lines = project_text.split('\n')
    current_project = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Skip common indicators that aren't project names
        if line.lower() in ['remote', 'on-site', 'hybrid', 'freelance', 'contract', 'full-time', 'part-time']:
            continue
        
        # Pattern 0: "Project Name â€” Description [Link]" (em-dash format like your projects)
        em_dash_match = re.match(r'^([^â€”\n]+?)\s*â€”\s*([^[\n]+?)(?:\s*\[.*?\])?\.?\s*$', line)
        if em_dash_match:
            project_name = em_dash_match.group(1).strip()
            project_desc = em_dash_match.group(2).strip()
            
            # Clean project name by removing any remaining link annotations
            project_name = re.sub(r'\[.*?\]', '', project_name).strip()
            
            # Validate this looks like a project name
            if (len(project_name) >= 3 and len(project_name) <= 60 and 
                project_name[0].isupper() and
                # Exclude section headers and achievement-related terms
                not any(keyword in project_name.lower() for keyword in ['achievements', 'achievement', 'extra', 'extracurricular', 'activities', 'awards', 'honors', 'experience', 'education', 'skills', 'certifications'])):
                
                # Save previous project
                if current_project:
                    projects.append(current_project)
                
                current_project = {
                    "name": project_name,
                    "description": project_desc
                }
                print(f"Found em-dash project: '{project_name}' â€” '{project_desc}'")
                continue
        
        # Pattern 1: "Project Name:" or "Project Name -" (clear project headers)
        header_match = re.match(r'^([^:â€¢\-*\n]+?)[:\-]\s*(.*?)$', line)
        if header_match:
            project_name = header_match.group(1).strip()
            project_desc = header_match.group(2).strip()
            
            # Clean project name by removing [Link] annotations
            project_name = re.sub(r'\[.*?\]', '', project_name).strip()
            
            # Validate this looks like a project name (not a description or section header)
            if (len(project_name) >= 3 and len(project_name) <= 60 and 
                not any(project_name.lower().startswith(verb) for verb in ['developed', 'created', 'built', 'implemented', 'designed', 'used', 'worked']) and
                project_name[0].isupper() and
                # Exclude section headers and achievement-related terms
                not any(keyword in project_name.lower() for keyword in ['achievements', 'achievement', 'extra', 'extracurricular', 'activities', 'awards', 'honors', 'experience', 'education', 'skills', 'certifications']) and
                # Exclude dated experiences (month/year patterns)
                not re.search(r'\b(?:january|february|march|april|may|june|july|august|september|october|november|december|jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\s+\d{4}\b', project_name.lower())):
                
                # Save previous project
                if current_project:
                    projects.append(current_project)
                
                current_project = {
                    "name": project_name,
                    "description": project_desc
                }
                print(f"Found project header: '{project_name}'")
                continue
        
        # Pattern 2: Bullet points or numbered items that look like project names
        bullet_match = re.match(r'^(?:\d+\.|\â€¢|\*|\-)\s*([^:â€¢\-*\n]+?)(?:[:\-]\s*(.*?))?$', line)
        if bullet_match:
            potential_name = bullet_match.group(1).strip()
            potential_desc = bullet_match.group(2).strip() if bullet_match.group(2) else ""
            
            # Clean potential name by removing [Link] annotations
            potential_name = re.sub(r'\[.*?\]', '', potential_name).strip()
            
            # Check if this looks like a project name (not a description or achievement)
            if (len(potential_name) <= 60 and 
                not any(potential_name.lower().startswith(verb) for verb in ['developed', 'created', 'built', 'implemented', 'designed', 'used', 'worked', 'integrated', 'deployed', 'received', 'awarded', 'achieved', 'won', 'earned', 'certified', 'completed', 'graduated', 'participated', 'attended', 'volunteered']) and
                potential_name[0].isupper() and
                # Additional check: contains project-type keywords OR doesn't contain achievement keywords
                (any(keyword in potential_name.lower() for keyword in ['app', 'website', 'system', 'platform', 'tool', 'dashboard', 'api', 'service', 'portal', 'application', 'project', 'software', 'game', 'simulator']) and
                 not any(keyword in potential_name.lower() for keyword in ['award', 'achievement', 'certificate', 'certification', 'degree', 'scholarship', 'honor', 'recognition', 'winner', 'prize', 'distinction', 'gpa', 'cgpa', 'grade', 'score', 'rank', 'experience', 'extra', 'extracurricular', 'activities']) and
                 # Exclude dated experiences
                 not re.search(r'\b(?:january|february|march|april|may|june|july|august|september|october|november|december|jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\s+\d{4}\b', potential_name.lower()))):
                
                # Save previous project
                if current_project:
                    projects.append(current_project)
                
                current_project = {
                    "name": potential_name,
                    "description": potential_desc
                }
                print(f"Found bulleted project: '{potential_name}'")
                continue
        
        # Pattern 3: Standalone project names (lines that don't start with bullets but look like titles)
        # Clean line by removing [Link] annotations first
        clean_line = re.sub(r'\[.*?\]', '', line).strip()
        
        if (len(clean_line) <= 60 and clean_line and clean_line[0].isupper() and 
            not any(clean_line.lower().startswith(verb) for verb in ['developed', 'created', 'built', 'implemented', 'designed', 'used', 'worked', 'integrated', 'deployed', 'received', 'awarded', 'achieved', 'won', 'earned', 'certified', 'graduated', 'completed', 'participated', 'volunteered', 'organized', 'led', 'managed', 'coordinated']) and
            (any(keyword in clean_line.lower() for keyword in ['app', 'website', 'system', 'platform', 'tool', 'dashboard', 'api', 'service', 'portal', 'application', 'project', 'software', 'game', 'simulator']) or
             # Allow names that don't contain action words and are reasonable length, but exclude extracurricular terms
             (len(clean_line.split()) <= 5 and not any(word in clean_line.lower() for word in ['award', 'achievement', 'certificate', 'certification', 'degree', 'scholarship', 'honor', 'recognition', 'winner', 'prize', 'distinction', 'club', 'society', 'team', 'captain', 'president', 'member', 'volunteer', 'community', 'event', 'competition', 'contest', 'tournament', 'league', 'association', 'achievements', 'extra', 'extracurricular', 'activities', 'experience', 'education', 'skills']))) and
            # Exclude dated experiences and section headers
            not re.search(r'\b(?:january|february|march|april|may|june|july|august|september|october|november|december|jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\s+\d{4}\b', clean_line.lower()) and
            # Exclude obvious section headers
            not any(clean_line.lower().strip() == header for header in ['achievements', 'achievements & extra', 'extracurricular', 'activities', 'experience', 'education', 'skills', 'awards', 'honors'])):
            
            # Save previous project
            if current_project:
                projects.append(current_project)
            
            current_project = {
                "name": clean_line,
                "description": ""
            }
            print(f"Found standalone project: '{clean_line}'")
            continue
        
        # Pattern 4: Description lines (add to current project)
        if current_project:
            # This line is a description for the current project
            if line.startswith('â€¢') or line.startswith('-') or line.startswith('*'):
                desc_text = line[1:].strip()
            else:
                desc_text = line
            
            if current_project["description"]:
                current_project["description"] += " " + desc_text
            else:
                current_project["description"] = desc_text
    
    # Don't forget the last project
    if current_project:
        projects.append(current_project)
    
    return projects

def extract_projects_from_full_text(text: str) -> List[Dict[str, Any]]:
    """Extract projects when no dedicated project section is found - very conservative approach"""
    projects = []
    
    # Only look for very explicit project patterns - avoid false positives from achievements/awards
    project_patterns = [
        # Pattern 0: "Project Name â€” Description [Link]" (em-dash format)
        r'(?m)^([A-Z][A-Za-z0-9\s,-]{2,50})\s*â€”\s*([^[\n]+?)(?:\s*\[.*?\])?\.?\s*$',
        # Pattern 1: "Project Name (Technology Stack)" - very reliable
        r'(?i)^([A-Z][A-Za-z0-9\s,-]{2,50})\s*\([^)]+(?:react|node|python|javascript|java|angular|vue|django|flask|spring|express|mongodb|sql|aws|docker|kubernetes|api|framework|library|technology|tech|stack)[^)]*\)(?:\s|$)',
        # Pattern 2: Lines explicitly mentioning "project" with name
        r'(?i)(?:^|\n)([A-Z][A-Za-z0-9\s,-]{2,60}?)\s*(?:project|app|application)(?:\s|$|\.)',
        # Pattern 3: "Developed/Created/Built [ProjectName] project/app/application"
        r'(?i)(?:developed|created|built|implemented|designed)\s+(?:a\s+|an\s+|the\s+)?([A-Z][A-Za-z0-9\s(),-]{2,50}?)\s+(?:project|application|app|website|system|platform)(?:\s|\.|\,)',
    ]
    
    # Be extra strict about what sections we search in
    lines = text.split('\n')
    in_projects_section = False
    current_section = ""
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # Check if we're entering a section
        section_headers = ['skills', 'experience', 'education', 'work experience', 'employment', 'achievements', 'awards', 'certifications', 'references', 'contact', 'summary', 'objective', 'languages', 'interests', 'hobbies', 'activities', 'volunteer', 'extracurricular', 'leadership', 'organizations']
        
        # Check if this line is a section header
        line_lower = line.lower().strip(':')
        if line_lower in section_headers:
            in_projects_section = False
            current_section = line_lower
            continue
        elif line_lower.startswith('project'):
            in_projects_section = True
            current_section = "projects"
            continue
        
        # Only extract from lines that are clearly not in other sections
        if not in_projects_section and current_section in section_headers:
            continue
            
        # Skip personal info lines
        if '@' in line or re.match(r'.*\(\d{3}\).*\d{4}', line):  # Email or phone
            continue
        if re.match(r'^[A-Z][a-z]+\s+[A-Z][a-z]+$', line.strip()):  # Likely a person's name
            continue
            
        # Apply patterns only if we're not clearly in another section
        for pattern in project_patterns:
            if pattern.startswith(r'(?m)^([A-Z][A-Za-z0-9\s,-]{2,50})\s*â€”'):
                # Special handling for em-dash pattern
                match = re.match(r'^([A-Z][A-Za-z0-9\s,-]{2,50})\s*â€”\s*([^[\n]+?)(?:\s*\[.*?\])?\.?\s*$', line)
                if match:
                    project_name = match.group(1).strip()
                    project_desc = match.group(2).strip()
                    
                    # Clean project name by removing [Link] annotations
                    project_name = re.sub(r'\[.*?\]', '', project_name).strip()
                    
                    # Very strict validation
                    if (len(project_name) >= 3 and len(project_name) <= 80 and
                        project_name[0].isupper() and
                        # Exclude achievement/section terms and dated experiences
                        not any(keyword in project_name.lower() for keyword in ['achievements', 'extra', 'extracurricular', 'activities', 'experience', 'education', 'skills', 'awards', 'honors']) and
                        not re.search(r'\b(?:january|february|march|april|may|june|july|august|september|october|november|december|jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\s+\d{4}\b', project_name.lower())):
                        
                        # Check if we already have this project (avoid duplicates)
                        existing_names = [p['name'].lower() for p in projects]
                        if project_name.lower() not in existing_names:
                            projects.append({
                                "name": project_name,
                                "description": project_desc
                            })
                            print(f"Found em-dash project: '{project_name}' â€” '{project_desc}'")
            else:
                matches = re.findall(pattern, line)
                for match in matches:
                    project_name = match.strip()
                    
                    # Clean project name by removing [Link] annotations
                    project_name = re.sub(r'\[.*?\]', '', project_name).strip()
                    
                    # Very strict validation
                    if (len(project_name) >= 3 and len(project_name) <= 80 and
                        not any(project_name.lower().startswith(word) for word in ['developed', 'created', 'built', 'implemented', 'designed', 'used', 'worked', 'responsible', 'received', 'awarded', 'achieved', 'won', 'earned', 'certified', 'the ', 'a ', 'an ']) and
                        project_name[0].isupper() and
                        # Exclude achievement/section terms and dated experiences
                        not any(keyword in project_name.lower() for keyword in ['achievements', 'extra', 'extracurricular', 'activities', 'experience', 'education', 'skills', 'awards', 'honors']) and
                        not re.search(r'\b(?:january|february|march|april|may|june|july|august|september|october|november|december|jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\s+\d{4}\b', project_name.lower())):
                        
                        # Check if we already have this project (avoid duplicates)
                        existing_names = [p['name'].lower() for p in projects]
                        if project_name.lower() not in existing_names:
                            projects.append({
                                "name": project_name,
                                "description": ""
                            })
                            print(f"Found project from conservative pattern: '{project_name}'")
    
    return projects
    
    return projects

def extract_additional_skills_from_achievements(text: str) -> List[str]:
    """Extract leadership, teamwork, and other soft skills from achievements and extracurricular sections"""
    additional_skills = []
    
    # Look for achievement and extracurricular sections
    achievement_patterns = [
        r'(?i)(?:achievements?|awards?|honors?|recognition)\s*[:\n]\s*(.*?)(?=\n\s*(?:SKILLS?|EXPERIENCE|EDUCATION|PROJECTS?|WORK\s+EXPERIENCE|EMPLOYMENT|CERTIFICATIONS?|REFERENCES?|CONTACT|SUMMARY|OBJECTIVE|LANGUAGES?|INTERESTS?|HOBBIES?)\s*[:\n]|$)',
        r'(?i)(?:extracurricular|activities|volunteer|leadership|organizations?)\s*[:\n]\s*(.*?)(?=\n\s*(?:SKILLS?|EXPERIENCE|EDUCATION|PROJECTS?|WORK\s+EXPERIENCE|EMPLOYMENT|ACHIEVEMENTS?|AWARDS?|CERTIFICATIONS?|REFERENCES?|CONTACT|SUMMARY|OBJECTIVE|LANGUAGES?|INTERESTS?|HOBBIES?)\s*[:\n]|$)'
    ]
    
    for pattern in achievement_patterns:
        matches = re.findall(pattern, text, re.DOTALL)
        for match in matches:
            section_text = match.strip().lower()
            
            # Extract soft skills from achievements/activities
            skill_indicators = {
                'leadership': ['president', 'captain', 'leader', 'head', 'coordinator', 'organizer', 'manager'],
                'teamwork': ['team', 'group', 'collaboration', 'cooperative', 'member'],
                'communication': ['presentation', 'speaking', 'debate', 'public speaking', 'communication'],
                'project management': ['organized', 'planned', 'coordinated', 'managed', 'executed'],
                'problem solving': ['solved', 'resolved', 'troubleshooting', 'analysis', 'strategy'],
                'time management': ['deadline', 'schedule', 'multitasking', 'prioritize'],
                'creativity': ['creative', 'innovative', 'design', 'artistic', 'original'],
                'analytical thinking': ['analysis', 'research', 'data', 'statistical', 'analytical'],
                'adaptability': ['adapted', 'flexible', 'versatile', 'diverse'],
                'mentoring': ['mentor', 'tutor', 'teach', 'guide', 'coach']
            }
            
            for skill, indicators in skill_indicators.items():
                if any(indicator in section_text for indicator in indicators):
                    if skill not in additional_skills:
                        additional_skills.append(skill.title())
    
    return additional_skills

def extract_experience(doc, text: str) -> List[Dict[str, Any]]:
    """Extract work experience from text"""
    # Ensure we have text to work with
    if not text and isinstance(doc, str):
        text = doc
    experience = []
    
    # Look for experience sections
    exp_section_pattern = r'(?i)(?:experience|work experience|employment)[:\n]([^\n]+(?:\n[^\n]+)*?)(?:\n\s*\n|\n\s*[A-Z]|$)'
    exp_matches = re.findall(exp_section_pattern, text)
    
    if exp_matches:
        for match in exp_matches:
            # Split by company names or dates
            exp_text = match.strip()
            exp_items = re.split(r'\n(?=[A-Z]|\d{4})', exp_text)
            
            for item in exp_items:
                if item.strip():
                    # Extract company name and position (first line)
                    lines = item.split('\n')
                    company_position = lines[0].strip()
                    
                    # Extract dates
                    date_pattern = r'(?:\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{4}\s*-\s*(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{4}|\d{4}\s*-\s*\d{4}|\d{4}\s*-\s*Present)'
                    dates = re.findall(date_pattern, item, re.IGNORECASE)
                    date_range = dates[0] if dates else ""
                    
                    # Extract description (remaining lines)
                    description = '\n'.join(lines[1:]).strip() if len(lines) > 1 else ""
                    
                    experience.append({
                        "company_position": company_position,
                        "date_range": date_range,
                        "description": description
                    })
    
    return experience

def extract_education(doc, text: str) -> List[Dict[str, Any]]:
    """Extract education from text"""
    # Ensure we have text to work with
    if not text and isinstance(doc, str):
        text = doc
    education = []
    
    # Look for education sections
    edu_section_pattern = r'(?i)education[:\n]([^\n]+(?:\n[^\n]+)*?)(?:\n\s*\n|\n\s*[A-Z]|$)'
    edu_matches = re.findall(edu_section_pattern, text)
    
    if edu_matches:
        for match in edu_matches:
            # Split by institution names or dates
            edu_text = match.strip()
            edu_items = re.split(r'\n(?=[A-Z]|\d{4})', edu_text)
            
            for item in edu_items:
                if item.strip():
                    # Extract institution name and degree (first line)
                    lines = item.split('\n')
                    institution_degree = lines[0].strip()
                    
                    # Extract dates
                    date_pattern = r'(?:\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{4}\s*-\s*(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{4}|\d{4}\s*-\s*\d{4}|\d{4}\s*-\s*Present)'
                    dates = re.findall(date_pattern, item, re.IGNORECASE)
                    date_range = dates[0] if dates else ""
                    
                    # Extract additional info (remaining lines)
                    additional_info = '\n'.join(lines[1:]).strip() if len(lines) > 1 else ""
                    
                    education.append({
                        "institution_degree": institution_degree,
                        "date_range": date_range,
                        "additional_info": additional_info
                    })
    
    return education

def extract_github_username(text: str) -> Optional[str]:
    """Extract GitHub username from text"""
    github_patterns = [
        r'github\.com/([\w.-]+)(?:/|\s|$)',
        r'github\.com/([\w.-]+)',
        r'github:\s*([\w.-]+)',
        r'github\s+username:\s*([\w.-]+)',
        r'github\s*[-:]?\s*([\w.-]+)',
        r'(?:^|\s)github\s*[:/]?\s*([\w.-]+)',
        r'@([\w.-]+)\s*\(?\s*github\s*\)?',
        r'https?://github\.com/([\w.-]+)'
    ]
    
    for pattern in github_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            username = match.group(1).strip('.')
            # Filter out common false positives
            if username.lower() not in ['com', 'www', 'http', 'https', 'github']:
                return username
    return None

def extract_twitter_username(text: str) -> Optional[str]:
    """Extract Twitter username from text"""
    twitter_patterns = [
        r'twitter\.com/([\w.-]+)(?:/|\s|$)',
        r'twitter\.com/([\w.-]+)',
        r'twitter:\s*@?([\w.-]+)',
        r'twitter\s+username:\s*@?([\w.-]+)',
        r'twitter\s*[-:]?\s*@?([\w.-]+)',
        r'(?:^|\s)twitter\s*[:/]?\s*@?([\w.-]+)',
        r'@([\w.-]+)\s*\(?\s*twitter\s*\)?',
        r'https?://twitter\.com/([\w.-]+)',
        r'x\.com/([\w.-]+)(?:/|\s|$)',
        r'https?://x\.com/([\w.-]+)'
    ]
    
    for pattern in twitter_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            username = match.group(1).strip('.')
            # Filter out common false positives and section headers
            false_positives = ['com', 'www', 'http', 'https', 'twitter', 'x', 'educa', 'education', 'tion', 'ion']
            if username.lower() not in false_positives and len(username) > 2:
                return username
    return None

def extract_linkedin_username(text: str) -> Optional[str]:
    """Extract LinkedIn username from text"""
    linkedin_patterns = [
        r'linkedin\.com/in/([\w.-]+)(?:/|\s|$)',
        r'linkedin\.com/in/([\w.-]+)',
        r'linkedin:\s*([\w.-]+)',
        r'linkedin\s+username:\s*([\w.-]+)',
        r'linkedin\s*[-:]?\s*([\w.-]+)',
        r'(?:^|\s)linkedin\s*[:/]?\s*([\w.-]+)',
        r'@([\w.-]+)\s*\(?\s*linkedin\s*\)?',
        r'https?://linkedin\.com/in/([\w.-]+)',
        r'https?://www\.linkedin\.com/in/([\w.-]+)'
    ]
    
    for pattern in linkedin_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            username = match.group(1).strip('.')
            # Filter out common false positives
            if username.lower() not in ['com', 'www', 'http', 'https', 'linkedin', 'in']:
                return username
    return None